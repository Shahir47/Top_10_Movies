{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This program takes serach term and returns top ten movies that is represented by the serach terms.  \n",
    "#The program is written following the Apache Spark Map Reduce framework. Cosine similarity has been utilized to create the serach funtionality.\n",
    "\n",
    "#Input to the program: A text file where each line represents a search term/terms.\n",
    "#Output: Top ten movies for each search term/terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366ae37f-a10a-4f78-83ee-00f298f6a4f6",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Requirement already satisfied: nltk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f0364b50-33be-4e5c-8adc-09104f342f2d/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f0364b50-33be-4e5c-8adc-09104f342f2d/lib/python3.9/site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f0364b50-33be-4e5c-8adc-09104f342f2d/lib/python3.9/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk) (1.1.1)\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe319811-40df-4453-972a-3859b4753e23",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b5d270-2c3f-48a8-b96a-3b2c1fb70dc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f99dd9d-e6e5-4ad0-9045-a0e5b1ca209b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movies_summary = sc.textFile('dbfs:/FileStore/tables/plot_summaries.txt').cache()\n",
    "movie_names = spark.read.option('header','false').option('inferSchema','true').option('delimiter','\\t').csv('dbfs:/FileStore/Assignment_1/movie_metadata.tsv') #reading the file in df format\n",
    "\n",
    "movie_name_rdd = movie_names.select(['_c0', '_c2']).rdd.map(lambda x : (str(x['_c0']), str(x['_c2']))) #converting from df to rdd format\n",
    "moviesRDD = movies_summary.map(lambda x : x.split(\"\\t\")).cache()\n",
    "tokensRDD = moviesRDD.map(lambda x : (x[0], x[1].split(\" \"))).cache()\n",
    "\n",
    "stop_words = stopwords.words('english') #will return list of stopwords in English\n",
    "tokens_processed = tokensRDD.map(lambda x : (x[0], [item.lower() for item in x[1] if item.lower() not in stop_words and item.isalpha()])).cache() #discarding stop words and only considering alphabetical words\n",
    "\n",
    "tf_single = tokens_processed.flatMap(lambda x : [((x[0], item), 1/len(x[1])) for item in x[1]]).reduceByKey(lambda x,y : x+y).cache()\n",
    "\n",
    "N = tokens_processed.count()\n",
    "idf = tf_single.map(lambda x : (x[0][1], 1)).reduceByKey(lambda x,y : x+y).map(lambda x : (x[0], 1+math.log(N/x[1]))).cache()\n",
    "\n",
    "#tf_idf_single\n",
    "tf_single_temp = tf_single.map(lambda x : (x[0][1], (x[0][0], x[1]))).cache()\n",
    "merged_single = tf_single_temp.join(idf).cache()\n",
    "tf_idf_single = merged_single.mapValues(lambda x : (x[0][0], x[0][1]*x[1])).cache() \n",
    "\n",
    "#tf_idf_sentence\n",
    "tf = tokens_processed.map(lambda x : (x[0], [(item, x[1].count(item)/len(x[1])) for item in set(x[1])])).cache()\n",
    "idf_dict = dict(idf.collect())\n",
    "tf_idf = tf.mapValues(lambda x : [(pair[0], idf_dict[pair[0]]*pair[1]) for pair in x]).cache() #tf_idf_single and tf_idf contains the same value in different format. It has been done for later convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746360cb-3c93-41a8-8c83-772f72124247",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_dict = {} # this holds tf-idf value for the query\n",
    "def input_tf_idf(test):\n",
    "    global test_dict\n",
    "\n",
    "    procss_ = [item.lower() for item in test if item not in stop_words and item.isalpha()]\n",
    "\n",
    "    test_rdd = sc.parallelize(procss_)\n",
    "    test_tf = test_rdd.map(lambda x : (x,1)).reduceByKey(lambda x,y : x+y).mapValues(lambda x : x/len(test))\n",
    "    test_tf_idf = test_tf.map(lambda x : (x[0], idf_dict.get(x[0], 0)*x[1]))\n",
    "    test_dict = dict(test_tf_idf.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f76072c-4199-4d42-aa3d-4c58a01aa655",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cosine_sim(list1): #this calculates cosine similarity between the documents from the corpus and query text\n",
    "    list1 = dict(list1)\n",
    "\n",
    "    query_list = []\n",
    "    doc_list = []\n",
    "\n",
    "    query = {key:value for key,value in sorted(test_dict.items())}\n",
    "    for key in query.keys():\n",
    "        doc_list.append(list1.get(key, 0))\n",
    "        query_list.append(query[key])\n",
    "    \n",
    "    q_val = 0\n",
    "    d_val = 0\n",
    "    dot = 0\n",
    "    result = 0\n",
    "\n",
    "    for i in range(len(query_list)):\n",
    "        dot += query_list[i]*doc_list[i]\n",
    "        q_val += query_list[i]*query_list[i]\n",
    "        d_val += doc_list[i]*doc_list[i]\n",
    "    denom = math.sqrt(q_val)+math.sqrt(d_val)\n",
    "    result = dot/denom\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e4b1d63-1f77-44af-b955-2d9af2ac9c23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main(input_txt):\n",
    "    \n",
    "    input_txt = word_tokenize(input_txt)\n",
    "    temp = ''\n",
    "\n",
    "    if(len(input_txt) != 1):\n",
    "        input_tf_idf(input_txt) #calculate tf-idf for the query\n",
    "        temp = tf_idf.mapValues(lambda x : cosine_sim(x)).sortBy(lambda x : -x[1])\n",
    "    else: #if query is a word simply return the top ten documents where the tf-idf for that value is the highest\n",
    "        temp = tf_idf_single.filter(lambda x : x[0] == input_txt[0].lower()).map(lambda x : (x[1][0], x[1][1])) \n",
    "\n",
    "    result = ''\n",
    "    if(temp.count() == 0):\n",
    "        result = \"No Movie Found\"\n",
    "    else:\n",
    "        #join the above result with movie_name_rdd to get the names\n",
    "        result = movie_name_rdd.join(temp).sortBy(lambda x : -x[1][1]) #sort with -x[1][1] allows descending sort by tf_idf values or the cosine values depending on the input query\n",
    "        result = list(result.map(lambda x : x[1]).take(10)) \n",
    "        # result.take(10)\n",
    "\n",
    "    print(f\"Search Result for: {' '.join(input_txt)}\")\n",
    "    i = 0\n",
    "    for item in result:\n",
    "        i += 1\n",
    "        print(f\"{i}. {item[0]}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9caee5dc-0474-4206-b06f-99ef99db4910",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Result for: Superhero movies batman superman flash dc\n",
      "1. Batman: Dead End\n",
      "2. Atom Man vs. Superman\n",
      "3. 1 Day\n",
      "4. Paper Doll\n",
      "5. Stamp Day for Superman\n",
      "6. Invaders from Space\n",
      "7. Superman/Batman: Apocalypse\n",
      "8. Atomic Rulers of the World\n",
      "9. All-Star Superman\n",
      "10. Superman/Batman: Public Enemies\n",
      "\n",
      "\n",
      "Search Result for: Dracula\n",
      "1. Vampira\n",
      "2. Hotel Transylvania\n",
      "3. Dracula\n",
      "4. Dracula 2000\n",
      "5. Dracula père et fils\n",
      "6. Dracula Has Risen from the Grave\n",
      "7. Dracula 3D\n",
      "8. Dracula\n",
      "9. Dracula A.D.1972\n",
      "10. Boo!\n",
      "\n",
      "\n",
      "Search Result for: Funny movie with action scene\n",
      "1. Kottarathil Kuttibhootham\n",
      "2. Lu and Bun\n",
      "3. The Major Lied 'Til Dawn\n",
      "4. Kote\n",
      "5. Treasure Buddies\n",
      "6. Giri\n",
      "7. Jaal\n",
      "8. Tiger\n",
      "9. The Daredevil Men\n",
      "10. Murder at Midnight\n",
      "\n",
      "\n",
      "Search Result for: Movies with alien and space\n",
      "1. Lighter Than Hare\n",
      "2. Paper Doll\n",
      "3. Prey\n",
      "4. The Attack of the Giant Moussaka\n",
      "5. 002 Operazione Luna\n",
      "6. Aunt Rose\n",
      "7. Space Chimps 2: Zartog Strikes Back\n",
      "8. The Falling\n",
      "9. Space Master X-7\n",
      "10. Metamorphosis: The Alien Factor\n",
      "\n",
      "\n",
      "Search Result for: Action movie with fights and war\n",
      "1. Eito Prem\n",
      "2. The Story of Two Women\n",
      "3. The Daredevil Men\n",
      "4. Giri\n",
      "5. Jaal\n",
      "6. Tiger\n",
      "7. Kote\n",
      "8. Treasure Buddies\n",
      "9. Billa No. 786\n",
      "10. Happy Land\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_file = sc.textFile('dbfs:/FileStore/Assignment_1/input_assign1.txt') #The search terms are in this file. In the console below you can see these search term.\n",
    "in_file = list(in_file.collect())\n",
    "\n",
    "for line in in_file:\n",
    "    main(line)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
